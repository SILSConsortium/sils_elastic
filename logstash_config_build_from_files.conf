input {
     file {
        path => "/opt/logstash/scripts/elk/output/circulation/circulation*.json"
	type => "circulation"
        start_position => "beginning"
        sincedb_path => "/dev/null"
	ignore_older => 8640000
        codec => "json"
     }
}


filter {

  #Remove these default fields that logstash provides that we don't care about.
  mutate {
	remove_field => [ "host", "path" ]
  }


  #We use the date filter to set the timestamp of the document to the time of the transaction.
  #Our SQL query puts that timestamp in a field called 'transactiondate' and it's formatted in ISO8601.  If yours differs, you'll need to change it.
  #After the transactiondate is set to the timestamp of the document, we just remove that field

  date {
         match => [ "transactiondate", "ISO8601" ]
         timezone => "America/Regina"
         remove_field => [ "transactiondate" ]
  }

}

#Output into monthly elasticsearch indexes.  
#Note the document_id is set to the transactionid field that is returned by Polaris for each row.

output {
	if [type] == "circulation" {
	      elasticsearch {
		  index => "sils-circ-%{+YYYY.MM}"
		  document_id => "%{transactionid}"
		  hosts => "172.16.2.50:9200"  #This is where Elasticsearch is running, likely on localhost:9200 for development
	      }
   	}
}
